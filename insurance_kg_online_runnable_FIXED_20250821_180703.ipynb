{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Insurance Knowledge Graph (Pure-Python, Online-Runnable)\n\nThis notebook is **self-contained**\u2014no external installs required.\nIt seeds data, builds a tiny in-memory KG, runs TF\u2013IDF retrieval, and answers with citations.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import os, csv, re, math\nfrom collections import defaultdict, Counter\n\nDATA_DIR = \"insurance_kg_demo\"\nos.makedirs(DATA_DIR, exist_ok=True)\n\nseed = {\n    \"policies.csv\": \"\"\"policy_number,effective_date,expiry_date,product,jurisdiction\nP123,2024-01-01,2024-12-31,Homeowners,CA\nP456,2024-06-01,2025-05-31,CommercialProperty,NY\n\"\"\",\n    \"coverages.csv\": \"\"\"policy_number,coverage_type,limit,deductible\nP123,Property,500000,1000\nP123,Liability,300000,0\nP456,Property,2000000,5000\n\"\"\",\n    \"exclusions.csv\": \"\"\"policy_number,exclusion_name,clause_ref,text\nP123,Flood,Clause 4.2,\"Flood losses are excluded unless a flood rider is attached.\"\nP123,Earth Movement,Clause 4.3,\"Earthquake and earth movement are excluded.\"\nP456,War,Clause 5.1,\"Losses caused by war or warlike operations are excluded.\"\n\"\"\",\n    \"riders.csv\": \"\"\"policy_number,rider_name,clause_ref,text\nP123,R-FLD,Clause 7.1,\"Adds flood coverage limit 100000 with $5000 deductible.\"\n\"\"\",\n    \"claims.csv\": \"\"\"claim_id,policy_number,date_of_loss,cause,amount,status\nCLM-987,P123,2019-11-12,Flood,18000,Closed\nCLM-654,P123,2023-02-10,Wind,3500,Closed\nCLM-222,P456,2024-12-01,Fire,120000,Open\n\"\"\",\n    \"risks.csv\": \"\"\"risk_id,type,score,model_version,policy_number\nRISK-1,Flood,0.72,rmv2,P123\nRISK-2,Earthquake,0.41,rmv2,P123\nRISK-3,Fire,0.65,rmv3,P456\n\"\"\",\n    \"regulations.csv\": \"\"\"jurisdiction,citation,section,text\nCA,CCR-Title10,\u00a72695.4,\"Fair claims settlement practices definitions include flood handling...\"\nNY,NYCRR-Insurance,\u00a7216.0,\"Unfair claims settlement standards for property claims...\"\n\"\"\",\n    # Use literal tabs for TSV\n    \"document_chunks.tsv\": \"\"\"doc_id\\tchunk_id\\tnode_type\\tnode_key\\tsource_uri\\tpage\\tclause\\ttext\nPOL-P123\\tc1\\tPolicy\\tP123\\ts3://bucket/POL-P123.pdf\\t12\\tClause 4.2\\tFlood losses are excluded unless a flood rider is attached.\nRID-R-FLD\\tc2\\tRider\\tR-FLD\\ts3://bucket/RID-R-FLD.pdf\\t2\\tClause 7.1\\tAdds flood coverage limit 100000 with $5000 deductible.\nCLAIM-987\\tc3\\tClaim\\tCLM-987\\ts3://bucket/CLAIM-987.txt\\t3\\t\\t2019 basement flood claim paid 18000 after assessment.\nREG-CA\\tc4\\tRegulation\\tCCR-Title10\\ts3://regs/CCR-Title10.html\\t18\\t\u00a72695.4\\tFair claims settlement practices on flood handling in California.\nPOL-P123\\tc5\\tPolicy\\tP123\\ts3://bucket/POL-P123.pdf\\t13\\tClause 4.3\\tEarth movement and earthquake are excluded.\nPOL-P456\\tc6\\tPolicy\\tP456\\ts3://bucket/POL-P456.pdf\\t20\\tClause 5.1\\tLosses caused by war or warlike operations are excluded.\n\"\"\",\n}\n\nfor name, content in seed.items():\n    path = os.path.join(DATA_DIR, name)\n    if not os.path.exists(path):\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n\nprint(\"Seeded sample data to\", DATA_DIR)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def read_csv(path):\n    rows = []\n    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n        for row in csv.DictReader(f):\n            rows.append(row)\n    return rows\n\ndef read_tsv(path):\n    rows = []\n    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f, delimiter=\"\\t\")\n        for row in reader:\n            rows.append(row)\n    return rows\n\npolicies    = read_csv(os.path.join(DATA_DIR, \"policies.csv\"))\ncoverages   = read_csv(os.path.join(DATA_DIR, \"coverages.csv\"))\nexclusions  = read_csv(os.path.join(DATA_DIR, \"exclusions.csv\"))\nriders      = read_csv(os.path.join(DATA_DIR, \"riders.csv\"))\nclaims      = read_csv(os.path.join(DATA_DIR, \"claims.csv\"))\nrisks       = read_csv(os.path.join(DATA_DIR, \"risks.csv\"))\nregulations = read_csv(os.path.join(DATA_DIR, \"regulations.csv\"))\ndoc_chunks  = read_tsv(os.path.join(DATA_DIR, \"document_chunks.tsv\"))\n\nprint(f\"Loaded: {len(policies)} policies, {len(doc_chunks)} chunks\")"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def nid_policy(policy_number): return f\"Policy:{policy_number.upper()}\"\ndef nid_coverage(policy_number, coverage_type): return f\"Coverage:{policy_number.upper()}:{coverage_type}\"\ndef nid_exclusion(policy_number, name): return f\"Exclusion:{policy_number.upper()}:{name}\"\ndef nid_rider(policy_number, name): return f\"Rider:{policy_number.upper()}:{name}\"\ndef nid_claim(claim_id): return f\"Claim:{claim_id.upper()}\"\ndef nid_risk(risk_id): return f\"Risk:{risk_id.upper()}\"\ndef nid_reg(j,c,s): return f\"Regulation:{j}:{c}:{s}\"\ndef nid_doc(doc_id, chunk_id): return f\"DocumentChunk:{doc_id}|{chunk_id}\"\n\nnodes = {}\nedges_out = defaultdict(list)\nedges_in  = defaultdict(list)\n\ndef add_node(node_id, typ, **props): nodes[node_id] = {\"type\": typ, \"props\": dict(props)}\ndef add_edge(src, rel, dst): edges_out[src].append((rel, dst)); edges_in[dst].append((rel, src))\n\nfor p in policies: add_node(nid_policy(p[\"policy_number\"]), \"Policy\", **p)\nfor c in coverages: add_node(nid_coverage(c[\"policy_number\"], c[\"coverage_type\"]), \"Coverage\", **c)\nfor e in exclusions: add_node(nid_exclusion(e[\"policy_number\"], e[\"exclusion_name\"]), \"Exclusion\", **e)\nfor r in riders: add_node(nid_rider(r[\"policy_number\"], r[\"rider_name\"]), \"Rider\", **r)\nfor c in claims: add_node(nid_claim(c[\"claim_id\"]), \"Claim\", **c)\nfor r in risks: add_node(nid_risk(r[\"risk_id\"]), \"Risk\", **r)\nfor rg in regulations: add_node(nid_reg(rg[\"jurisdiction\"], rg[\"citation\"], rg[\"section\"]), \"Regulation\", **rg)\nfor d in doc_chunks: add_node(nid_doc(d[\"doc_id\"], d[\"chunk_id\"]), \"DocumentChunk\", **d)\n\nfor c in coverages: add_edge(nid_policy(c[\"policy_number\"]), \"HAS_COVERAGE\", nid_coverage(c[\"policy_number\"], c[\"coverage_type\"]))\nfor e in exclusions: add_edge(nid_policy(e[\"policy_number\"]), \"HAS_EXCLUSION\", nid_exclusion(e[\"policy_number\"], e[\"exclusion_name\"]))\nfor r in riders: add_edge(nid_policy(r[\"policy_number\"]), \"HAS_RIDER\", nid_rider(r[\"policy_number\"], r[\"rider_name\"]))\nfor c in claims: add_edge(nid_claim(c[\"claim_id\"]), \"AGAINST_POLICY\", nid_policy(c[\"policy_number\"]))\nfor r in risks: add_edge(nid_risk(r[\"risk_id\"]), \"ASSESSMENT_OF\", nid_policy(r[\"policy_number\"]))\n\nfor d in doc_chunks:\n    nk, nt = d[\"node_key\"], d[\"node_type\"]\n    if nt == \"Policy\": add_edge(nid_doc(d[\"doc_id\"], d[\"chunk_id\"]), \"EVIDENCES\", nid_policy(nk))\n    elif nt == \"Rider\": add_edge(nid_doc(d[\"doc_id\"], d[\"chunk_id\"]), \"EVIDENCES\", nid_rider(\"P123\", nk))\n    elif nt == \"Claim\": add_edge(nid_doc(d[\"doc_id\"], d[\"chunk_id\"]), \"EVIDENCES\", nid_claim(nk))\n    elif nt == \"Regulation\": add_edge(nid_doc(d[\"doc_id\"], d[\"chunk_id\"]), \"EVIDENCES\", nid_policy(\"P123\"))\n\nprint(f\"In-memory KG ready: {len(nodes)} nodes, {sum(len(v) for v in edges_out.values())} edges\")"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "TOKEN_RE = re.compile(r\"[A-Za-z0-9#]+\")\ndef tokenize(text): return [t.lower() for t in TOKEN_RE.findall(text or \"\")]\n\ncorpus = [{\"id\": f\"{d['doc_id']}:{d['chunk_id']}\", \"text\": d[\"text\"], \"meta\": d} for d in doc_chunks]\ndf = Counter(); [df.update(set(tokenize(doc[\"text\"]))) for doc in corpus]\nN = len(corpus) or 1\nidf = {t: (math.log((N+1)/(df_t+1)) + 1.0) for t, df_t in df.items()}\n\ndef tf(text):\n    c = Counter(tokenize(text)); tot = sum(c.values()) or 1\n    return {t: c[t]/tot for t in c}\n\ndef tfidf_vec(text):\n    tfq = tf(text)\n    return {t: tfq[t] * idf.get(t, 0.0) for t in tfq}\n\ndef cosine_sim(v1, v2):\n    keys = set(v1) | set(v2)\n    dot = sum(v1.get(k, 0.0) * v2.get(k, 0.0) for k in keys)\n    n1 = math.sqrt(sum(x*x for x in v1.values())) or 1e-9\n    n2 = math.sqrt(sum(x*x for x in v2.values())) or 1e-9\n    return dot / (n1 * n2)\n\ndef top_k_chunks(query, k=6):\n    qv = tfidf_vec(query); scored = []\n    for doc in corpus:\n        dv = tfidf_vec(doc[\"text\"])\n        s = cosine_sim(qv, dv)\n        if s > 0: scored.append((s, doc))\n    scored.sort(reverse=True, key=lambda x: x[0])\n    return [d for s, d in scored[:k]]"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "POLICY_RE = re.compile(r\"\\bP\\d+\\b\", re.IGNORECASE)\nCLAIM_RE  = re.compile(r\"\\bCLM-\\d+\\b\", re.IGNORECASE)\n\ndef extract_anchors(question):\n    policies = POLICY_RE.findall(question) or []\n    claims = CLAIM_RE.findall(question) or []\n    keywords = set(tokenize(question))\n    intents = {\n        \"exclusion\": any(k in keywords for k in [\"exclude\",\"exclusion\",\"excluded\",\"exclusions\"]),\n        \"rider\": any(k in keywords for k in [\"rider\",\"riders\"]),\n        \"claim\": any(k in keywords for k in [\"claim\",\"claims\",\"loss\",\"losses\"]),\n        \"coverage\": any(k in keywords for k in [\"cover\",\"coverage\",\"covered\",\"limit\",\"deductible\"]),\n        \"reg\": any(k in keywords for k in [\"regulation\",\"regulations\",\"jurisdiction\",\"law\",\"section\"]),\n    }\n    risks = [k for k in [\"flood\",\"earth\",\"earthquake\",\"wind\",\"fire\",\"war\"] if k in keywords]\n    return {\"policies\": [p.upper() for p in policies], \"claims\": [c.upper() for c in claims], \"intents\": intents, \"risk_terms\": risks}\n\ndef expand_graph_from_policy(policy_number, max_hops=2):\n    start = nid_policy(policy_number)\n    visited = {start}; frontier = [start]; results = {start}; hops = 0\n    while frontier and hops < max_hops:\n        nxt = []\n        for u in frontier:\n            for rel, v in edges_out.get(u, []):\n                results.add(v)\n                if v not in visited: visited.add(v); nxt.append(v)\n            for rel, v in edges_in.get(u, []):\n                results.add(v)\n                if v not in visited: visited.add(v); nxt.append(v)\n        frontier = nxt; hops += 1\n    ev = []\n    for nid in results:\n        for rel, src in edges_in.get(nid, []):\n            if rel == \"EVIDENCES\" and nodes.get(src, {}).get(\"type\") == \"DocumentChunk\":\n                ev.append(nodes[src][\"props\"])\n    return results, ev\n\ndef retrieve(question):\n    anchors = extract_anchors(question)\n    node_ids = set(); evidence = []\n    for p in anchors[\"policies\"]:\n        nids, ev = expand_graph_from_policy(p)\n        node_ids |= nids; evidence.extend(ev)\n    seen = set(); ev_uniq = []\n    for d in evidence:\n        key = (d.get(\"doc_id\"), d.get(\"chunk_id\"))\n        if key not in seen: seen.add(key); ev_uniq.append(d)\n    text_hits = [doc[\"meta\"] for doc in top_k_chunks(question, k=6)]\n    seen = set(); merged = []\n    for d in text_hits + ev_uniq:\n        key = (d.get(\"doc_id\"), d.get(\"chunk_id\"))\n        if key not in seen: seen.add(key); merged.append(d)\n    return anchors, node_ids, merged[:10]\n\ndef format_citation(d):\n    return f\"{d.get('doc_id','')}:{d.get('clause','')} p{d.get('page','')}\".strip()\n\ndef answer(question, anchors, node_ids, chunks):\n    parts = []; citations = []\n    qlow = question.lower()\n    def find(term): return [d for d in chunks if term in (d.get(\"text\") or \"\").lower()]\n    if anchors[\"intents\"][\"exclusion\"]:\n        parts.append(\"**Exclusions**:\")\n        for nid in node_ids:\n            if isinstance(nid, str) and nid.startswith(\"Exclusion:\"):\n                exname = nid.split(\":\")[-1]\n                sn = find(exname.lower())\n                if sn:\n                    cite = format_citation(sn[0]); parts.append(f\"- {exname} \u2014 see {cite}\"); citations.append(cite)\n    if anchors[\"intents\"][\"rider\"] or anchors[\"intents\"][\"coverage\"]:\n        parts.append(\"**Riders & Coverage Notes**:\")\n        for nid in node_ids:\n            if isinstance(nid, str) and nid.startswith(\"Rider:\"):\n                rname = nid.split(\":\")[-1]\n                sn = find(rname.lower())\n                if sn:\n                    cite = format_citation(sn[0]); parts.append(f\"- {rname}: {sn[0]['text']} ({cite})\"); citations.append(cite)\n    if anchors[\"intents\"][\"claim\"]:\n        parts.append(\"**Claims**:\")\n        for nid in node_ids:\n            if isinstance(nid, str) and nid.startswith(\"Claim:\"):\n                clid = nid.split(\":\")[-1]\n                sn = find(clid.lower())\n                if sn:\n                    cite = format_citation(sn[0]); parts.append(f\"- {clid}: see {cite}\"); citations.append(cite)\n    if anchors[\"risk_terms\"]:\n        parts.append(\"**Risk-specific note**:\")\n        for term in anchors[\"risk_terms\"]:\n            sn = find(term)\n            if sn:\n                cite = format_citation(sn[0]); parts.append(f\"- Evidence for '{term}': {cite}\"); citations.append(cite)\n    if not parts:\n        parts.append(\"**Top Evidence Excerpts** (fallback):\")\n        for d in chunks[:5]:\n            cite = format_citation(d); parts.append(f\"- {d['text']} ({cite})\"); citations.append(cite)\n    uniq = []; seen = set()\n    for c in citations:\n        if c not in seen: seen.add(c); uniq.append(c)\n    return \"\\n\".join(parts), uniq"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "examples = [\n    \"List all exclusions for policy P123 and indicate if any riders restore coverage.\",\n    \"After adding R-FLD, does P123 cover flood and what deductible applies?\",\n    \"Show claims on P123 related to flood and the relevant clauses.\",\n    \"Are there any California regulations affecting flood claim handling for P123?\"\n]\n\nfor q in examples:\n    anchors, node_ids, chunks = retrieve(q)\n    ans, cites = answer(q, anchors, node_ids, chunks)\n    print(\"Q:\", q)\n    print(ans)\n    print(\"Citations:\", cites[:5], \"...\")\n    print(\"-\"*80)\n\nq = \"After adding R-FLD, does P123 cover flood and what deductible applies?\"\nanchors, node_ids, chunks = retrieve(q)\nans, cites = answer(q, anchors, node_ids, chunks)\nassert \"5000\" in ans, \"Expected deductible '5000' not found in answer.\"\nprint(\"Smoke test passed.\")"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}